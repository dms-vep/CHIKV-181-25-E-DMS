""" 
Snakemake pipeline for the intrahost variant analysis of CHIKV.

Author: Will Hannon
"""

#### ----------------------- Imports ----------------------- ####

import pandas as pd 
from os.path import join, basename
from Bio import Entrez
from io import StringIO

#### -------------------- Configuration -------------------- ####

configfile: "configuration/pipeline.yaml"

# Load and filter the SRA runs based on the config
sra_df = pd.read_csv(config["fetch"]["sra_table"])
accessions = sra_df[sra_df["total_count"] >= config["fetch"]["min_total_count"]]["acc"].tolist()
# remove accessions in the exclude list
if "exclude" in config["fetch"]:
    accessions = [acc for acc in accessions if acc not in config["fetch"]["exclude"]]
print(f"Found {len(accessions)} accessions to download.")

#### ---------------------- Rule Files --------------------- ####

include: "rules/functions.smk"
include: "rules/paired.smk"
include: "rules/single.smk"

#### ----------------------- Targets ----------------------- ####

rule all:
    input:
        join(config['variants_dir'], "variants.ivar.csv"),
        join(config['depth_dir'], "merged.depth"),
        join(config['consensus_dir'], "consensus.fa")

#### ------------------------ Rules ------------------------ ####

rule extract_reference_from_genbank:
    """
    Extract the reference genome from a GenBank file.
    """
    input: 
        gb = config['reference']
    output: 
        fa = join(config['ref_dir'], "reference.fa"),
        gff = join(config['ref_dir'], "reference.gff")
    run:
        from Bio import SeqIO
        from Bio.SeqRecord import SeqRecord
        import BCBio.GFF
        # Read the GenBank file with SeqIO
        seq = SeqIO.read(input.gb, "genbank")
        # Write the reference genome to a FASTA file
        SeqIO.write([seq], output.fa, "fasta")
        # Write the feature annotations to a GFF file
        with open(output.gff, "w") as gff:
            BCBio.GFF.write([seq], gff)


rule align_with_minimap:
    input:
        reads=get_processed_fastqs,
        reference = join(config['ref_dir'], "reference.fa"),
    output:
        bam = join(config['align_dir'], "{accession}", "{accession}.bam"),
        flagstat = join(config['qc_dir'], "{accession}", "{accession}.flagstat")
    log: join(config['log_dir'], "align_with_minimap", "align_with_minimap_{accession}.log")
    params:
        preset = lambda wildcards: get_minimap_preset(wildcards.accession)
    conda: 'intrahost'
    threads: config['align']['threads']
    shell:
        """
        minimap2 -ax {params.preset} \
            --cs \
            -t {threads} \
            {input.reference} \
            {input.reads} 2> {log} | \
        samtools view -b - | \
        samtools sort -@ {threads} -o {output.bam} -
        samtools index {output.bam}
        samtools flagstat -@ {threads} {output.bam} > {output.flagstat}
        """


rule calculate_read_depth:
    input:
        bam = join(config['align_dir'], "{accession}", "{accession}.bam"),
    output:
        depth = join(config['depth_dir'], "{accession}", "{accession}.depth")
    conda: 'intrahost'
    shell:
        """
        samtools depth -aa -Q 0 {input.bam} > {output.depth}
        sed -i "s/$/\t{wildcards.accession}/" {output}
        """



rule aggregate_read_depth:
    input: 
        expand(join(config['depth_dir'], "{accession}", "{accession}.depth"), accession=accessions)
    output: 
        depth = join(config['depth_dir'], "merged.depth"),
        header = temp(join(config['depth_dir'], "merged.depth.tmp"))
    shell:
        """
        cat {input} > {output.header}

        awk 'BEGIN{{print "REF\tPOS\tDP\tAccession"}}1' {output.header} > {output.depth}
        """
        

rule create_consensus_with_ivar:
    input: 
        bam = join(config['align_dir'], "{accession}", "{accession}.bam"),
    output: 
        join(config['consensus_dir'], "{accession}", "{accession}.fa")
    params: prefix = join(config['consensus_dir'], "{accession}", "{accession}")
    conda: 'intrahost'
    shell:
        """
        samtools mpileup -d 1000 -A -Q 0 {input.bam} | ivar consensus -p {params.prefix} -q 20 -t 0
        """


rule aggregate_consensus_sequences:
    input: 
        expand(join(config['consensus_dir'], "{accession}", "{accession}.fa"), accession=accessions)
    output: 
        join(config['consensus_dir'], "consensus.fa")
    run:
        with open(output[0], 'w') as outfile:
            for file in input:
                with open(file, 'r') as infile:
                    outfile.write(infile.read())


rule call_variants_with_ivar:
    input: 
        bam = join(config['align_dir'], "{accession}", "{accession}.bam"),
        reference = join(config['ref_dir'], "reference.fa"),
        gff = join(config['ref_dir'], "reference.gff")
    output:
        join(config['variants_dir'], "{accession}", "{accession}.ivar.tsv")
    params:
        prefix = join(config['variants_dir'], "{accession}", "{accession}.ivar"),
        max_depth = config['variants']['max_depth'],
        min_qual = config['variants']['min_qual'],
        min_freq = config['variants']['min_freq'],
        min_depth = config['variants']['min_depth']
    conda: 'intrahost'
    log: join(config['log_dir'], "call_variants_with_ivar", "call_variants_with_ivar_{accession}.log")
    shell:
        """
        samtools mpileup \
            -aa -A -B \
            -d {params.max_depth} \
            -Q 0 \
            {input.bam} \
        | ivar variants \
            -p {params.prefix} \
            -q {params.min_qual} \
            -t {params.min_freq} \
            -m {params.min_depth} \
            -r {input.reference} \
            -g {input.gff} \
            &>> {log}
        """


rule aggregate_ivar_variants:
    input: 
        expand(join(config['variants_dir'], "{accession}", "{accession}.ivar.tsv"), accession=accessions),

    output: 
        variants = join(config['variants_dir'], "variants.ivar.csv"),
    run:
        dfs = []
        for file in input:
            accession, _, _ = basename(file).split('.')
            df = pd.read_csv(file, sep='\t')
            df['Accession'] = accession
            dfs.append(df)
        df = pd.concat(dfs)
        df.to_csv(output.variants, index=False)
